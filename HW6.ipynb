{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model,Sequential,load_model\n",
    "from keras.layers import Dense, Embedding, Input,LSTM,Bidirectional, GlobalMaxPool1D,GlobalMaxPooling1D,\\\n",
    "GlobalAveragePooling1D,Dropout,CuDNNLSTM,SpatialDropout1D,CuDNNGRU,BatchNormalization, Activation,merge, Lambda\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import optimizers\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import os\n",
    "\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DIR_MAC= '/Users/haipengwu/Codings/Machine Learning/HW6'\n",
    "DIR_1080=\"C:\\\\Users\\\\WIN10\\\\Codings\\\\Machine Learning\\\\HW6\"\n",
    "\n",
    "DIR = DIR_1080\n",
    "\n",
    "MAX_LEN = 200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(DIR,'train.csv'))\n",
    "test = pd.read_csv(os.path.join(DIR,'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "words = stopwords.words('english')\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "word_count_dict = defaultdict(int)\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "clean_word_dict = {}\n",
    "with open(os.path.join(DIR,'cleanwords.txt'), 'r', encoding='utf-8') as cl:\n",
    "    for line in cl:\n",
    "        line = line.strip('\\n')\n",
    "        typo, correct = line.split(',')\n",
    "        clean_word_dict[typo] = correct\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "\n",
    "    stemmed_text =[]\n",
    "    lmtz_text=[]\n",
    "    no_stop_text = []\n",
    "    normal_text = [] # just normal text, with stopwords, not stemmed nor ltmz words\n",
    "    cleaned_text =\" \"\n",
    "    \n",
    "        \n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    for typo, correct in clean_word_dict.items():\n",
    "        text = re.sub(typo, \" \" + correct + \" \", text)\n",
    "                \n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \", text) # delete all of the non-Latin words\n",
    "    text = text.lower() \n",
    "\n",
    "    for i in text.split():                \n",
    "#         if i not in words:\n",
    "        normal_text.append(i)\n",
    "#             lmtz_text.append(lmtzr.lemmatize(i)) #try lemmatizer\n",
    "#             stemmed_text.append(stemmer.stem(i)) #try stemmer\n",
    "        \n",
    "\n",
    "    return cleaned_text.join(normal_text) # <- str must contain join, otherwise it is still empty...\n",
    "\n",
    "\n",
    "\n",
    "if not (os.path.exists(os.path.join(DIR,'cleaned_train.csv')) and os.path.exists(os.path.join(DIR,'cleaned_test.csv'))):\n",
    "    \n",
    "    print(\"Text Cleaning Start...\")\n",
    "    \n",
    "\n",
    "    list_sentences_train = train[\"comment_text\"].fillna(\"no comment\").values\n",
    "    list_sentences_test = test[\"comment_text\"].fillna(\"no comment\").values\n",
    "\n",
    "    comments = [clean_text(text) for text in list_sentences_train] \n",
    "    \n",
    "    test_comments=[clean_text(text) for text in list_sentences_test]\n",
    "\n",
    "    train['comment_text'] = comments\n",
    "    test['comment_text'] = test_comments\n",
    "\n",
    "    train.to_csv(os.path.join(DIR,'cleaned_train.csv'), index=False)\n",
    "    test.to_csv(os.path.join(DIR,'cleaned_test.csv'), index=False)\n",
    "    \n",
    "    print(\"Text Cleaning Finished...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(DIR,'cleaned_train.csv'))\n",
    "test = pd.read_csv(os.path.join(DIR,'cleaned_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_sentence_train = train[\"comment_text\"].fillna(\"no comment\").values\n",
    "list_sentence_test = test[\"comment_text\"].fillna(\"no comment\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train = train[list_classes].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the training data: give each word of a sentence a token(unique number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 200000\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words = VOCAB_SIZE )\n",
    "tokenizer.fit_on_texts(list(list_sentence_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentence_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentence_test)\n",
    "\n",
    "# Set the vocabulary size by the length of word_index of the tokenized list\n",
    "VOCAB_SIZE = len(tokenizer.word_index)+1\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the pretrained word vectors into embeddings_index dictionary. In this dict, words are the keywords, values are the pretrained word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open('E:\\\\Downloads\\\\glove.840B.300d\\\\glove.840B.300d.txt', encoding='utf-8') as f:\n",
    "# with open(DIR+'\\\\glove\\\\glove.6B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word =values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:],dtype='float32')\n",
    "            embeddings_index[word]=coefs\n",
    "        except ValueError:\n",
    "            pass\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a weight matrix for words in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_VECTOR_LEN = 300\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, WORD_VECTOR_LEN))\n",
    "for word, i in tokenizer.word_index.items(): # items() return the pair: the keyword and its value\n",
    "    embedding_vector = embeddings_index.get(word) # this is the word vector of the word in the training data\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad each tokenized sequence to make each input X_train the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(list_tokenized_train,maxlen=MAX_LEN)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test,maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Label Process : split the X_train, the first 15000 entries are used for validation, while the rest are used for training. This is for the pseudo label purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pl_capture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[:500,]\n",
    "y_val = y_train[:500,]\n",
    "X_train = X_train[500:,]\n",
    "y_train = y_train[500:,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the toxic comments classification training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECURRENT_UNITS = 10\n",
    "\n",
    "def tcc_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(VOCAB_SIZE, WORD_VECTOR_LEN, weights = [embedding_matrix], input_shape=(MAX_LEN, ),trainable = False))\n",
    "\n",
    "    model.add(SpatialDropout1D(0.1))    \n",
    "\n",
    "    model.add(Bidirectional(CuDNNGRU(RECURRENT_UNITS, return_sequences=True))) \n",
    "    model.add(Bidirectional(CuDNNGRU(RECURRENT_UNITS, return_sequences=True))) \n",
    "\n",
    "    model.add(GlobalMaxPool1D()) \n",
    "\n",
    "    model.add(Dropout(0.1))   \n",
    "    model.add(Dense(144, activation=\"relu\"))\n",
    "    model.add(Dense(6, activation=\"sigmoid\"))\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "### an alternative solution which uses some concatencations\n",
    "def tcc_model_av_rnn():\n",
    "    \n",
    "\n",
    "    input_layer = Input(shape=(MAX_LEN,))\n",
    "    embedding_layer = Embedding(VOCAB_SIZE,\n",
    "                                WORD_VECTOR_LEN,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_LEN,\n",
    "                                trainable=False)(input_layer)\n",
    "    embedding_layer = SpatialDropout1D(0.25)(embedding_layer)\n",
    "\n",
    "    rnn_1 = Bidirectional(CuDNNGRU(RECURRENT_UNITS, return_sequences=True))(embedding_layer)\n",
    "    rnn_2 = Bidirectional(CuDNNGRU(RECURRENT_UNITS, return_sequences=True))(rnn_1)\n",
    "    x = concatenate([rnn_1, rnn_2], axis=2)\n",
    "\n",
    "    last = Lambda(lambda t: t[:, -1], name='last')(x)\n",
    "    maxpool = GlobalMaxPooling1D()(x)\n",
    "#     attn = AttentionWeightedAverage()(x)\n",
    "    average = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    all_views = concatenate([last, maxpool, average], axis=1)\n",
    "    x = Dropout(0.5)(all_views)\n",
    "    x = Dense(144, activation=\"relu\")(x)\n",
    "    output_layer = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 320\n",
    "EPOCHS = 10\n",
    "\n",
    "model = tcc_model()\n",
    "# model = tcc_model_av_rnn()\n",
    "\n",
    "adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "model.compile(loss='binary_crossentropy',optimizer=adam_optimizer,metrics=['accuracy'])\n",
    "# set tensorboard\n",
    "NAME = \"toxic_comments_classification-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "#set modelcheckpoint\n",
    "\n",
    "MODELSAVENAME = \"\\\\model\\\\TCC_Best.h5\"\n",
    "checkpoint = ModelCheckpoint(DIR+\"{}\".format(MODELSAVENAME),monitor='val_acc', \\\n",
    "                                                      verbose=1, save_best_only=True, mode='max') # saves only the best ones\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, shuffle=True, validation_split=0,validation_data=(X_val,y_val), callbacks=[tensorboard,checkpoint])\n",
    "\n",
    "#load the best model\n",
    "model = load_model(MODELSAVENAME)\n",
    "\n",
    "# predict and use the pseudo labels\n",
    "print(\"predict and use the pseudo labels...\")\n",
    "y_test_pl = model.predict(X_test)\n",
    "# y_test_pl = np.around(y_test_pl)\n",
    "print(\"...finished\")\n",
    "X_train_2nd = np.concatenate((X_train,X_test),axis = 0)\n",
    "y_train_2nd = np.concatenate((y_train,y_test_pl),axis = 0)\n",
    "print(X_train_2nd.shape, y_train_2nd.shape)\n",
    "print(\"training the model with the testing data + pseudo labels...\")\n",
    "print(\"loading the best model...\")\n",
    "MODELSAVENAME_PL = \"TCC_Best_Pseudo_Labeling.h5\"\n",
    "\n",
    "checkpoint_pl = ModelCheckpoint(\"C:\\\\Users\\\\WIN10\\\\Codings\\\\Machine Learning\\\\HW6\\\\model\\\\model\\\\{}\".format(MODELSAVENAME_PL), monitor='val_acc', \\\n",
    "                                                      verbose=1, save_best_only=True, mode='max') # saves only the best ones\n",
    "\n",
    "model.fit(X_train_2nd, y_train_2nd, batch_size=BATCH_SIZE, epochs=EPOCHS, shuffle=True, validation_split=0,validation_data=(X_val,y_val), callbacks=[tensorboard,checkpoint_pl])\n",
    "\n",
    "#load the best PL model\n",
    "model = load_model(MODELSAVENAME_PL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test,batch_size = BATCH_SIZE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(os.path.join(DIR,'sample_submission.csv'))\n",
    "sample_submission [list_classes] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission_wu.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "## 1. Pretrained word embedding is very very important, which give +0.6 accuracy with same other configurations. The best result comes from glove.840B.300d.txt, which can be found at https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "## 2. Text Cleaning is important, use Regular Expression: exclude the non-English words;\n",
    "\n",
    "## 3. The model is not neccessarily complicated. Embedding, BiGRU x2, Maxpooling1D x1, Dense x2, and some dropout in between, that's it.\n",
    "\n",
    "## 4. Pseudo Labelling works a little bit, need further investigation\n",
    "\n",
    "## 5. Easy to overfit, so Early stoppings (both normal training and PL training) are important, how to do it automatically? need further investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
